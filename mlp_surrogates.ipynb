{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f121bd9-127c-492e-8fd1-81b505ed1a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tanh network on MNIST using cuda:4\n",
      "Using surrogate gamma calculation method\n",
      "Epoch 1 completed, Avg Loss: 0.4870\n",
      "Epoch 2 completed, Avg Loss: 0.3298\n",
      "Epoch 3 completed, Avg Loss: 0.2595\n",
      "Epoch 4 completed, Avg Loss: 0.2184\n",
      "Epoch 5 completed, Avg Loss: 0.1904\n",
      "Epoch 6 completed, Avg Loss: 0.1699\n",
      "Epoch 7 completed, Avg Loss: 0.1539\n",
      "Epoch 8 completed, Avg Loss: 0.1410\n",
      "Epoch 9/100, Iter 1000, Loss: 0.0390\n",
      "Epoch 9 completed, Avg Loss: 0.1306\n",
      "Epoch 10 completed, Avg Loss: 0.1229\n",
      "Epoch 11 completed, Avg Loss: 0.1158\n",
      "Epoch 12 completed, Avg Loss: 0.1094\n",
      "Epoch 13 completed, Avg Loss: 0.1036\n",
      "Epoch 14 completed, Avg Loss: 0.0994\n",
      "Epoch 15 completed, Avg Loss: 0.0952\n",
      "Epoch 16 completed, Avg Loss: 0.0914\n",
      "Epoch 17/100, Iter 2000, Loss: 0.0467\n",
      "Epoch 17 completed, Avg Loss: 0.0881\n",
      "Epoch 18 completed, Avg Loss: 0.0849\n",
      "Epoch 19 completed, Avg Loss: 0.0823\n",
      "Epoch 20 completed, Avg Loss: 0.0797\n",
      "Epoch 21 completed, Avg Loss: 0.0775\n",
      "Epoch 22 completed, Avg Loss: 0.0752\n",
      "Epoch 23 completed, Avg Loss: 0.0734\n",
      "Epoch 24 completed, Avg Loss: 0.0717\n",
      "Epoch 25 completed, Avg Loss: 0.0698\n",
      "Epoch 26/100, Iter 3000, Loss: 0.0382\n",
      "Epoch 26 completed, Avg Loss: 0.0686\n",
      "Epoch 27 completed, Avg Loss: 0.0673\n",
      "Epoch 28 completed, Avg Loss: 0.0657\n",
      "Epoch 29 completed, Avg Loss: 0.0643\n",
      "Epoch 30 completed, Avg Loss: 0.0635\n",
      "Epoch 31 completed, Avg Loss: 0.0625\n",
      "Epoch 32 completed, Avg Loss: 0.0611\n",
      "Epoch 33 completed, Avg Loss: 0.0603\n",
      "Epoch 34/100, Iter 4000, Loss: 0.0205\n",
      "Epoch 34 completed, Avg Loss: 0.0593\n",
      "Epoch 35 completed, Avg Loss: 0.0583\n",
      "Epoch 36 completed, Avg Loss: 0.0575\n",
      "Epoch 37 completed, Avg Loss: 0.0566\n",
      "Epoch 38 completed, Avg Loss: 0.0558\n",
      "Epoch 39 completed, Avg Loss: 0.0553\n",
      "Epoch 40 completed, Avg Loss: 0.0546\n",
      "Epoch 41 completed, Avg Loss: 0.0539\n",
      "Epoch 42 completed, Avg Loss: 0.0532\n",
      "Epoch 43/100, Iter 5000, Loss: 0.0320\n",
      "Epoch 43 completed, Avg Loss: 0.0528\n",
      "Epoch 44 completed, Avg Loss: 0.0523\n",
      "Epoch 45 completed, Avg Loss: 0.0517\n",
      "Epoch 46 completed, Avg Loss: 0.0511\n",
      "Epoch 47 completed, Avg Loss: 0.0507\n",
      "Epoch 48 completed, Avg Loss: 0.0501\n",
      "Epoch 49 completed, Avg Loss: 0.0495\n",
      "Epoch 50 completed, Avg Loss: 0.0489\n",
      "Epoch 51/100, Iter 6000, Loss: 0.0313\n",
      "Epoch 51 completed, Avg Loss: 0.0484\n",
      "Epoch 52 completed, Avg Loss: 0.0479\n",
      "Epoch 53 completed, Avg Loss: 0.0478\n",
      "Epoch 54 completed, Avg Loss: 0.0473\n",
      "Epoch 55 completed, Avg Loss: 0.0469\n",
      "Epoch 56 completed, Avg Loss: 0.0465\n",
      "Epoch 57 completed, Avg Loss: 0.0460\n",
      "Epoch 58 completed, Avg Loss: 0.0458\n",
      "Epoch 59 completed, Avg Loss: 0.0456\n",
      "Epoch 60/100, Iter 7000, Loss: 0.0150\n",
      "Epoch 60 completed, Avg Loss: 0.0452\n",
      "Epoch 61 completed, Avg Loss: 0.0447\n",
      "Epoch 62 completed, Avg Loss: 0.0444\n",
      "Epoch 63 completed, Avg Loss: 0.0440\n",
      "Epoch 64 completed, Avg Loss: 0.0438\n",
      "Epoch 65 completed, Avg Loss: 0.0435\n",
      "Epoch 66 completed, Avg Loss: 0.0433\n",
      "Epoch 67 completed, Avg Loss: 0.0429\n",
      "Epoch 68/100, Iter 8000, Loss: 0.0135\n",
      "Epoch 68 completed, Avg Loss: 0.0426\n",
      "Epoch 69 completed, Avg Loss: 0.0422\n",
      "Epoch 70 completed, Avg Loss: 0.0419\n",
      "Epoch 71 completed, Avg Loss: 0.0418\n",
      "Epoch 72 completed, Avg Loss: 0.0416\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import gamma calculation functions\n",
    "import math\n",
    "from functools import lru_cache\n",
    "from typing import Callable, Tuple, Dict, Any, Iterable\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "from numpy.polynomial.legendre import leggauss\n",
    "\n",
    "# ----------------------------\n",
    "# Gamma calculation utilities\n",
    "# ----------------------------\n",
    "\n",
    "SQRT2   = math.sqrt(2.0)\n",
    "SQRTPI  = math.sqrt(math.pi)\n",
    "SQRT2PI = math.sqrt(2.0 * math.pi)\n",
    "INV_SQRT2PI = 1.0 / SQRT2PI\n",
    "LOG_SQRT2PI = 0.5 * math.log(2.0 * math.pi)\n",
    "\n",
    "def phi(x: np.ndarray | float) -> np.ndarray | float:\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    return np.exp(-0.5 * x * x) * INV_SQRT2PI\n",
    "\n",
    "def Phi(x: np.ndarray | float) -> np.ndarray | float:\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    return 0.5 * (1.0 + np.vectorize(math.erf)(x / SQRT2))\n",
    "\n",
    "def log_Phi(x: np.ndarray | float) -> np.ndarray | float:\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    out = np.empty_like(x)\n",
    "    lo = x < -10.0\n",
    "    hi = x >  10.0\n",
    "    md = ~(lo | hi)\n",
    "    if np.any(lo):\n",
    "        xm = x[lo]\n",
    "        corr = 1.0 - 1.0 / (xm * xm)\n",
    "        corr = np.maximum(corr, 1e-12)\n",
    "        out[lo] = (-0.5 * xm * xm) - LOG_SQRT2PI - np.log(np.abs(xm)) + np.log(corr)\n",
    "    if np.any(md):\n",
    "        xm = x[md]\n",
    "        out[md] = np.log(np.clip(Phi(xm), 1e-300, 1.0))\n",
    "    if np.any(hi):\n",
    "        out[hi] = 0.0\n",
    "    return float(out) if out.shape == () else out\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def _leg_nodes(n: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    x, w = leggauss(n)\n",
    "    return x.astype(np.float64), w.astype(np.float64)\n",
    "\n",
    "def _gl_chunk_expectation(fn_z: Callable[[np.ndarray], np.ndarray], L: float, n: int) -> float:\n",
    "    x, w = _leg_nodes(n)\n",
    "    z = L * x\n",
    "    vals = fn_z(z).astype(np.float64) * phi(z)\n",
    "    return float(L * (w @ vals))\n",
    "\n",
    "def normal_expectation_integral(fn_z: Callable[[np.ndarray], np.ndarray],\n",
    "                                L_init: float = 10.0, n_init: int = 128,\n",
    "                                tol_rel: float = 1e-9, tol_abs: float = 1e-12,\n",
    "                                max_L_steps: int = 5, max_n_steps: int = 6) -> float:\n",
    "    L = float(L_init)\n",
    "    target = None\n",
    "    eps = 1e-16\n",
    "    I_prev = None\n",
    "\n",
    "    for _ in range(max_L_steps):\n",
    "        n = int(n_init)\n",
    "        I_prev = _gl_chunk_expectation(fn_z, L=L, n=n)\n",
    "        converged_n = False\n",
    "        for _ in range(max_n_steps):\n",
    "            n *= 2\n",
    "            I_curr = _gl_chunk_expectation(fn_z, L=L, n=n)\n",
    "            if abs(I_curr - I_prev) <= max(tol_abs, tol_rel * (abs(I_curr) + eps)):\n",
    "                target = I_curr\n",
    "                converged_n = True\n",
    "                break\n",
    "            I_prev = I_curr\n",
    "        if converged_n:\n",
    "            I_Lplus = _gl_chunk_expectation(fn_z, L=L+2.0, n=max(n_init, n//2))\n",
    "            if abs(I_Lplus - target) <= max(tol_abs, tol_rel * (abs(I_Lplus) + eps)):\n",
    "                return float(I_Lplus)\n",
    "        L += 2.0\n",
    "    return float(I_prev if I_prev is not None else 0.0)\n",
    "\n",
    "# Activation functions and derivatives\n",
    "def relu(u: np.ndarray) -> np.ndarray:\n",
    "    return np.maximum(u, 0.0)\n",
    "\n",
    "def relu_prime(u: np.ndarray) -> np.ndarray:\n",
    "    return (u > 0.0).astype(np.float64)\n",
    "\n",
    "def gelu(u: np.ndarray) -> np.ndarray:\n",
    "    return u * Phi(u)\n",
    "\n",
    "def gelu_prime(u: np.ndarray) -> np.ndarray:\n",
    "    return Phi(u) + u * phi(u)\n",
    "\n",
    "def tanh_fn(u: np.ndarray) -> np.ndarray:\n",
    "    return np.tanh(u)\n",
    "\n",
    "def tanh_prime(u: np.ndarray) -> np.ndarray:\n",
    "    t = np.tanh(u)\n",
    "    return 1.0 - t * t\n",
    "\n",
    "def sigmoid(u: np.ndarray) -> np.ndarray:\n",
    "    u = np.asarray(u, dtype=np.float64)\n",
    "    out = np.empty_like(u)\n",
    "    pos = u >= 0\n",
    "    neg = ~pos\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-u[pos]))\n",
    "    e = np.exp(u[neg])\n",
    "    out[neg] = e / (1.0 + e)\n",
    "    return out\n",
    "\n",
    "def sigmoid_prime(u: np.ndarray) -> np.ndarray:\n",
    "    s = sigmoid(u)\n",
    "    return s * (1.0 - s)\n",
    "\n",
    "def swish(u: np.ndarray) -> np.ndarray:\n",
    "    s = sigmoid(u)\n",
    "    return u * s\n",
    "\n",
    "def swish_prime(u: np.ndarray) -> np.ndarray:\n",
    "    s = sigmoid(u)\n",
    "    return s + u * s * (1.0 - s)\n",
    "\n",
    "def leaky_relu(u: np.ndarray, alpha_neg: float = 0.01) -> np.ndarray:\n",
    "    return np.where(u > 0.0, u, alpha_neg * u)\n",
    "\n",
    "def leaky_relu_prime(u: np.ndarray, alpha_neg: float = 0.01) -> np.ndarray:\n",
    "    return np.where(u > 0.0, 1.0, alpha_neg)\n",
    "\n",
    "def elu(u: np.ndarray, alpha: float = 1.0) -> np.ndarray:\n",
    "    return np.where(u > 0.0, u, alpha * (np.exp(u) - 1.0))\n",
    "\n",
    "def elu_prime(u: np.ndarray, alpha: float = 1.0) -> np.ndarray:\n",
    "    return np.where(u > 0.0, 1.0, alpha * np.exp(u))\n",
    "\n",
    "def selu(u: np.ndarray) -> np.ndarray:\n",
    "    SELU_ALPHA = 1.6732632423543772\n",
    "    SELU_LAMBDA = 1.0507009873554805\n",
    "    return SELU_LAMBDA * elu(u, alpha=SELU_ALPHA)\n",
    "\n",
    "def selu_prime(u: np.ndarray) -> np.ndarray:\n",
    "    SELU_ALPHA = 1.6732632423543772\n",
    "    SELU_LAMBDA = 1.0507009873554805\n",
    "    return SELU_LAMBDA * elu_prime(u, alpha=SELU_ALPHA)\n",
    "\n",
    "def mish(u: np.ndarray) -> np.ndarray:\n",
    "    sp = np.log1p(np.exp(-np.abs(u))) + np.maximum(u, 0.0)  # softplus\n",
    "    return u * np.tanh(sp)\n",
    "\n",
    "def mish_prime(u: np.ndarray) -> np.ndarray:\n",
    "    sp = np.log1p(np.exp(-np.abs(u))) + np.maximum(u, 0.0)  # softplus\n",
    "    t = np.tanh(sp)\n",
    "    return t + u * sigmoid(u) * (1.0 - t * t)\n",
    "\n",
    "def moments_via_integral(m: float, s: float, act: Callable[..., np.ndarray], \n",
    "                         dact: Callable[..., np.ndarray], integral_opts: Dict[str, Any] | None = None,\n",
    "                         **act_kwargs) -> Tuple[float, float, float, float]:\n",
    "    integral_opts = integral_opts or {}\n",
    "    def fz(z: np.ndarray) -> np.ndarray:\n",
    "        u = m + s * z\n",
    "        return act(u, **act_kwargs)\n",
    "    def fpz(z: np.ndarray) -> np.ndarray:\n",
    "        u = m + s * z\n",
    "        return dact(u, **act_kwargs)\n",
    "    mu  = normal_expectation_integral(fz,  **integral_opts)\n",
    "    Ef2 = normal_expectation_integral(lambda z: fz(z) ** 2, **integral_opts)\n",
    "    Efp = normal_expectation_integral(fpz, **integral_opts)\n",
    "    Efp2= normal_expectation_integral(lambda z: fpz(z) ** 2, **integral_opts)\n",
    "    return mu, Ef2, Efp, Efp2\n",
    "\n",
    "def gammas_from_moments(a: float, mu: float, Ef2: float, Efp: float, Efp2: float,\n",
    "                        eps_var: float = 1e-12) -> Tuple[float, float, float]:\n",
    "    var_f = max(Ef2 - mu * mu, eps_var)\n",
    "    sigma = math.sqrt(var_f)\n",
    "    scale = (a / sigma) ** 2\n",
    "    g2 = Efp2\n",
    "    g1 = scale * g2\n",
    "    g0 = scale * (Efp ** 2)\n",
    "    return g0, g1, g2\n",
    "\n",
    "# Closed-form moments for surrogate calculations\n",
    "def relu_moments_closed(m: float, s: float) -> Tuple[float, float, float, float]:\n",
    "    t = m / s\n",
    "    Ph = Phi(t); ph = phi(t)\n",
    "    mu  = s * ph + m * Ph\n",
    "    Ef2 = (s * s + m * m) * Ph + m * s * ph\n",
    "    Efp = Ph\n",
    "    Efp2= Ph\n",
    "    return float(mu), float(Ef2), float(Efp), float(Efp2)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def _herm_nodes(n: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    x, w = hermgauss(n)\n",
    "    return x.astype(np.float64), w.astype(np.float64)\n",
    "\n",
    "def gh_expectation(fn: Callable[[np.ndarray], np.ndarray], n_nodes: int = 128) -> float:\n",
    "    \"\"\"E[fn(Z)] with Z~N(0,1) via Gauss–Hermite\"\"\"\n",
    "    x, w = _herm_nodes(n_nodes)\n",
    "    z = math.sqrt(2.0) * x\n",
    "    vals = fn(z).astype(np.float64)\n",
    "    return float((w @ vals) / SQRTPI)\n",
    "\n",
    "def get_exact_gamma_function(activation: str):\n",
    "    \"\"\"Get exact gamma calculation function for given activation\"\"\"\n",
    "    activation_map = {\n",
    "        'relu': (relu, relu_prime, {}),\n",
    "        'gelu': (gelu, gelu_prime, {}),\n",
    "        'tanh': (tanh_fn, tanh_prime, {}),\n",
    "        'sigmoid': (sigmoid, sigmoid_prime, {}),\n",
    "        'swish': (swish, swish_prime, {}),\n",
    "        'silu': (swish, swish_prime, {}),\n",
    "        'leaky_relu': (leaky_relu, leaky_relu_prime, {'alpha_neg': 0.01}),\n",
    "        'elu': (elu, elu_prime, {'alpha': 1.0}),\n",
    "        'selu': (selu, selu_prime, {}),\n",
    "        'mish': (mish, mish_prime, {})\n",
    "    }\n",
    "    \n",
    "    if activation not in activation_map:\n",
    "        raise ValueError(f\"Activation {activation} not supported for gamma calculation\")\n",
    "    \n",
    "    act_func, act_prime_func, default_params = activation_map[activation]\n",
    "    \n",
    "    def gamma_calculator(a: float, b: float) -> Tuple[float, float, float]:\n",
    "        if a <= 1e-8:  # Avoid numerical issues\n",
    "            return 0.0, 0.0, 0.0\n",
    "        \n",
    "        m, s = a * b, a\n",
    "        integral_opts = {'L_init': 10.0, 'n_init': 192, 'tol_rel': 1e-9, 'tol_abs': 1e-12}\n",
    "        mu, Ef2, Efp, Efp2 = moments_via_integral(m, s, act_func, act_prime_func, \n",
    "                                                   integral_opts=integral_opts, **default_params)\n",
    "        return gammas_from_moments(a, mu, Ef2, Efp, Efp2)\n",
    "    \n",
    "    return gamma_calculator\n",
    "\n",
    "def get_surrogate_gamma_function(activation: str, gh_nodes: int = 160):\n",
    "    \"\"\"Get surrogate gamma calculation function for given activation\"\"\"\n",
    "    activation_lower = activation.lower()\n",
    "    \n",
    "    def gamma_calculator(a: float, b: float) -> Tuple[float, float, float]:\n",
    "        if a <= 1e-8:  # Avoid numerical issues\n",
    "            return 0.0, 0.0, 0.0\n",
    "        \n",
    "        m, s = a * b, a\n",
    "        \n",
    "        # Use closed-form for ReLU, Gauss-Hermite for others\n",
    "        if activation_lower == 'relu':\n",
    "            mu, Ef2, Efp, Efp2 = relu_moments_closed(m, s)\n",
    "        elif activation_lower == 'gelu':\n",
    "            mu  = gh_expectation(lambda z: gelu(m + s*z), n_nodes=gh_nodes)\n",
    "            Ef2 = gh_expectation(lambda z: gelu(m + s*z)**2, n_nodes=gh_nodes)\n",
    "            Efp = gh_expectation(lambda z: gelu_prime(m + s*z), n_nodes=gh_nodes)\n",
    "            Efp2= gh_expectation(lambda z: gelu_prime(m + s*z)**2, n_nodes=gh_nodes)\n",
    "        elif activation_lower == 'tanh':\n",
    "            mu  = gh_expectation(lambda z: tanh_fn(m + s*z), n_nodes=gh_nodes)\n",
    "            Ef2 = gh_expectation(lambda z: tanh_fn(m + s*z)**2, n_nodes=gh_nodes)\n",
    "            Efp = gh_expectation(lambda z: tanh_prime(m + s*z), n_nodes=gh_nodes)\n",
    "            Efp2= gh_expectation(lambda z: tanh_prime(m + s*z)**2, n_nodes=gh_nodes)\n",
    "        elif activation_lower == 'sigmoid':\n",
    "            mu  = gh_expectation(lambda z: sigmoid(m + s*z), n_nodes=gh_nodes)\n",
    "            Ef2 = gh_expectation(lambda z: sigmoid(m + s*z)**2, n_nodes=gh_nodes)\n",
    "            Efp = gh_expectation(lambda z: sigmoid_prime(m + s*z), n_nodes=gh_nodes)\n",
    "            Efp2= gh_expectation(lambda z: sigmoid_prime(m + s*z)**2, n_nodes=gh_nodes)\n",
    "        elif activation_lower in ['swish', 'silu']:\n",
    "            mu  = gh_expectation(lambda z: swish(m + s*z), n_nodes=gh_nodes)\n",
    "            Ef2 = gh_expectation(lambda z: swish(m + s*z)**2, n_nodes=gh_nodes)\n",
    "            Efp = gh_expectation(lambda z: swish_prime(m + s*z), n_nodes=gh_nodes)\n",
    "            Efp2= gh_expectation(lambda z: swish_prime(m + s*z)**2, n_nodes=gh_nodes)\n",
    "        elif activation_lower == 'leaky_relu':\n",
    "            alpha_neg = 0.01\n",
    "            mu  = gh_expectation(lambda z: leaky_relu(m + s*z, alpha_neg=alpha_neg), n_nodes=gh_nodes)\n",
    "            Ef2 = gh_expectation(lambda z: leaky_relu(m + s*z, alpha_neg=alpha_neg)**2, n_nodes=gh_nodes)\n",
    "            Efp = gh_expectation(lambda z: leaky_relu_prime(m + s*z, alpha_neg=alpha_neg), n_nodes=gh_nodes)\n",
    "            Efp2= gh_expectation(lambda z: leaky_relu_prime(m + s*z, alpha_neg=alpha_neg)**2, n_nodes=gh_nodes)\n",
    "        elif activation_lower == 'elu':\n",
    "            alpha = 1.0\n",
    "            mu  = gh_expectation(lambda z: elu(m + s*z, alpha=alpha), n_nodes=gh_nodes)\n",
    "            Ef2 = gh_expectation(lambda z: elu(m + s*z, alpha=alpha)**2, n_nodes=gh_nodes)\n",
    "            Efp = gh_expectation(lambda z: elu_prime(m + s*z, alpha=alpha), n_nodes=gh_nodes)\n",
    "            Efp2= gh_expectation(lambda z: elu_prime(m + s*z, alpha=alpha)**2, n_nodes=gh_nodes)\n",
    "        elif activation_lower == 'selu':\n",
    "            mu  = gh_expectation(lambda z: selu(m + s*z), n_nodes=gh_nodes)\n",
    "            Ef2 = gh_expectation(lambda z: selu(m + s*z)**2, n_nodes=gh_nodes)\n",
    "            Efp = gh_expectation(lambda z: selu_prime(m + s*z), n_nodes=gh_nodes)\n",
    "            Efp2= gh_expectation(lambda z: selu_prime(m + s*z)**2, n_nodes=gh_nodes)\n",
    "        elif activation_lower == 'mish':\n",
    "            mu  = gh_expectation(lambda z: mish(m + s*z), n_nodes=gh_nodes)\n",
    "            Ef2 = gh_expectation(lambda z: mish(m + s*z)**2, n_nodes=gh_nodes)\n",
    "            Efp = gh_expectation(lambda z: mish_prime(m + s*z), n_nodes=gh_nodes)\n",
    "            Efp2= gh_expectation(lambda z: mish_prime(m + s*z)**2, n_nodes=gh_nodes)\n",
    "        else:\n",
    "            raise ValueError(f\"Activation {activation} not supported for surrogate gamma calculation\")\n",
    "        \n",
    "        return gammas_from_moments(a, mu, Ef2, Efp, Efp2)\n",
    "    \n",
    "    return gamma_calculator\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=3072, hidden_sizes=[512, 256], num_classes=10, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.activations, self.pre_activations = {}, {}\n",
    "        \n",
    "        # Activation functions\n",
    "        act_map = {\n",
    "            'relu': nn.ReLU(), 'tanh': nn.Tanh(), 'sigmoid': nn.Sigmoid(),\n",
    "            'gelu': nn.GELU(), 'swish': nn.SiLU(), 'silu': nn.SiLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.01), 'elu': nn.ELU(), 'selu': nn.SELU(),\n",
    "            'mish': nn.Mish(), 'prelu': nn.PReLU()\n",
    "        }\n",
    "        self.activation_fn = act_map[activation]\n",
    "        self.activation_name = activation\n",
    "        \n",
    "        # Build layers\n",
    "        sizes = [input_size] + hidden_sizes + [num_classes]\n",
    "        self.layers = nn.ModuleList([nn.Linear(sizes[i], sizes[i+1]) for i in range(len(sizes)-1)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        self.activations, self.pre_activations = {}, {}\n",
    "        \n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            pre_act = layer(x)\n",
    "            self.pre_activations[i] = pre_act.detach()\n",
    "            x = self.activation_fn(pre_act)\n",
    "            self.activations[i] = x.detach()\n",
    "        \n",
    "        return self.layers[-1](x)\n",
    "    \n",
    "    def activation_derivative_squared(self, z):\n",
    "        \"\"\"Compute f'(z)^2 for various activation functions\"\"\"\n",
    "        derivatives = {\n",
    "            'relu': lambda z: (z > 0).float(),\n",
    "            'tanh': lambda z: 1 - torch.tanh(z)**2,\n",
    "            'sigmoid': lambda z: torch.sigmoid(z) * (1 - torch.sigmoid(z)),\n",
    "            'gelu': lambda z: 0.5 * (1 + torch.tanh((2/torch.pi)**0.5 * (z + 0.044715 * z**3))) + \n",
    "                             z * (1 - torch.tanh((2/torch.pi)**0.5 * (z + 0.044715 * z**3))**2) * \n",
    "                             (2/torch.pi)**0.5 * (1 + 3 * 0.044715 * z**2),\n",
    "            'swish': lambda z: torch.sigmoid(z) + z * torch.sigmoid(z) * (1 - torch.sigmoid(z)),\n",
    "            'silu': lambda z: torch.sigmoid(z) + z * torch.sigmoid(z) * (1 - torch.sigmoid(z)),\n",
    "            'selu': lambda z: torch.where(z > 0, torch.ones_like(z) * (1.0507**2), (1.0507 * 1.6733 * torch.exp(z))**2),\n",
    "            'leaky_relu': lambda z: torch.where(z > 0, torch.ones_like(z), 0.01 * torch.ones_like(z)),\n",
    "            'elu': lambda z: torch.where(z > 0, torch.ones_like(z), torch.exp(z)),\n",
    "            'mish': lambda z: torch.sigmoid(z) * torch.tanh(torch.nn.functional.softplus(z)),\n",
    "            'prelu': lambda z: torch.where(z > 0, torch.ones_like(z), 0.25 * torch.ones_like(z))\n",
    "        }\n",
    "        return derivatives.get(self.activation_name, derivatives['relu'])(z)**2\n",
    "\n",
    "def compute_max_correlation(arr):\n",
    "    \"\"\"Compute mean of max correlations per row from upper triangular matrix\"\"\"\n",
    "    corr = np.corrcoef(arr.T)\n",
    "    upper_tri = np.triu(corr, k=1)\n",
    "    max_corrs = [np.nanmax(upper_tri[i, i+1:]) for i in range(len(upper_tri)-1)]\n",
    "    return np.nanmean(max_corrs) if max_corrs else 0.0\n",
    "\n",
    "def compute_duplicated_units_fraction(arr, threshold=0.95):\n",
    "    \"\"\"Compute fraction of units that have high correlation (>threshold) with other units\"\"\"\n",
    "    corr = np.corrcoef(arr.T)  # arr.T so we get correlation between columns (units)\n",
    "    n_units = corr.shape[0]\n",
    "    if n_units <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    duplicated_count = 0\n",
    "    for i in range(n_units):\n",
    "        # Look at row i, upper triangular part (j > i)\n",
    "        upper_tri_row = corr[i, i+1:]\n",
    "        if len(upper_tri_row) > 0 and np.any(np.abs(upper_tri_row) > threshold):\n",
    "            duplicated_count += 1\n",
    "    \n",
    "    return duplicated_count / n_units\n",
    "\n",
    "def compute_duplicated_units_fraction(arr, threshold=0.95):\n",
    "    \"\"\"Compute fraction of units that have high correlation (>threshold) with other units\"\"\"\n",
    "    corr = np.corrcoef(arr.T)  # arr.T so we get correlation between columns (units)\n",
    "    n_units = corr.shape[0]\n",
    "    if n_units <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    duplicated_count = 0\n",
    "    for i in range(n_units):\n",
    "        # Look at row i, upper triangular part (j > i)\n",
    "        upper_tri_row = corr[i, i+1:]\n",
    "        if len(upper_tri_row) > 0 and np.any(np.abs(upper_tri_row) > threshold):\n",
    "            duplicated_count += 1\n",
    "    \n",
    "    return duplicated_count / n_units\n",
    "\n",
    "def calculate_metrics(model, trainloader, device, gamma_mode='exact', gh_nodes=160):\n",
    "    \"\"\"Calculate all metrics including gamma values for each layer\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get gamma calculator for this activation\n",
    "    try:\n",
    "        if gamma_mode == 'exact':\n",
    "            gamma_calc = get_exact_gamma_function(model.activation_name)\n",
    "        elif gamma_mode == 'surrogate':\n",
    "            gamma_calc = get_surrogate_gamma_function(model.activation_name, gh_nodes=gh_nodes)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid gamma_mode: {gamma_mode}. Use 'exact' or 'surrogate'\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: {e}\")\n",
    "        gamma_calc = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images, _ = next(iter(trainloader))\n",
    "        _ = model(images.to(device))\n",
    "        \n",
    "        metrics = {}\n",
    "        for layer_idx in model.pre_activations:\n",
    "            pre_act = model.pre_activations[layer_idx]\n",
    "            post_act = model.activations[layer_idx]\n",
    "            \n",
    "            # Convert to numpy\n",
    "            pre_np, post_np = pre_act.cpu().numpy(), post_act.cpu().numpy()\n",
    "            \n",
    "            # Calculate derivative squared\n",
    "            deriv_sq = model.activation_derivative_squared(pre_act).cpu().numpy()\n",
    "            \n",
    "            # Calculate normalized means (Sharpe-like ratios): mean/std per channel\n",
    "            def calc_normalized_stats(arr):\n",
    "                channel_means = np.mean(arr, axis=0)  # Mean per channel\n",
    "                channel_stds = np.std(arr, axis=0)    # Std per channel\n",
    "                # Avoid division by zero: set ratio to 0 where std is very small\n",
    "                normalized = np.where(channel_stds > 1e-8, channel_means / channel_stds, 0)\n",
    "                return np.mean(normalized), np.std(normalized)\n",
    "            \n",
    "            pre_norm_mean, pre_norm_std = calc_normalized_stats(pre_np)\n",
    "            \n",
    "            # Calculate gamma values if available\n",
    "            gamma0_mean = gamma1_mean = gamma2_mean = 0.0\n",
    "            if gamma_calc is not None:\n",
    "                # For each channel, compute gamma using std as 'a' and mean/std as 'b'\n",
    "                channel_means = np.mean(pre_np, axis=0)\n",
    "                channel_stds = np.std(pre_np, axis=0)\n",
    "                \n",
    "                gamma0_vals, gamma1_vals, gamma2_vals = [], [], []\n",
    "                for i in range(len(channel_stds)):\n",
    "                    std_val = float(channel_stds[i])\n",
    "                    mean_std_ratio = float(channel_means[i] / channel_stds[i]) if channel_stds[i] > 1e-8 else 0.0\n",
    "                    \n",
    "                    if std_val > 1e-8:  # Only compute if std is reasonable\n",
    "                        g0, g1, g2 = gamma_calc(std_val, mean_std_ratio)\n",
    "                        gamma0_vals.append(g0)\n",
    "                        gamma1_vals.append(g1)\n",
    "                        gamma2_vals.append(g2)\n",
    "                \n",
    "                # Take mean of gamma values across channels\n",
    "                gamma0_mean = np.mean(gamma0_vals) if gamma0_vals else 0.0\n",
    "                gamma1_mean = np.mean(gamma1_vals) if gamma1_vals else 0.0\n",
    "                gamma2_mean = np.mean(gamma2_vals) if gamma2_vals else 0.0\n",
    "            \n",
    "            metrics[layer_idx] = {\n",
    "                'pre_corr': compute_max_correlation(pre_np),\n",
    "                'post_corr': compute_max_correlation(post_np),\n",
    "                'pre_norm_mean': pre_norm_mean,\n",
    "                'pre_norm_std': pre_norm_std,\n",
    "                'pre_std': np.mean(np.std(pre_np, axis=0)),\n",
    "                'post_std': np.mean(np.std(post_np, axis=0)),\n",
    "                'deriv_mean': np.mean(deriv_sq),\n",
    "                'deriv_std': np.std(np.mean(deriv_sq, axis=0)),\n",
    "                'gamma0_mean': gamma0_mean,\n",
    "                'gamma1_mean': gamma1_mean,\n",
    "                'gamma2_mean': gamma2_mean,\n",
    "                'duplicated_fraction': compute_duplicated_units_fraction(pre_np, threshold=0.95)\n",
    "            }\n",
    "    return metrics\n",
    "\n",
    "def plot_metrics(metrics_log, config):\n",
    "    \"\"\"Create 8-panel plot with separate gamma plots and duplicated units\"\"\"\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(12, 20))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Extract data\n",
    "    iterations = [m[0] for m in metrics_log]\n",
    "    num_layers = len(metrics_log[0][1])\n",
    "    # Progression color scheme: cool (blue) to warm (red) through the network layers\n",
    "    colors = plt.cm.coolwarm(np.linspace(0.1, 0.9, num_layers))\n",
    "    \n",
    "    # Plot configurations: (metric_pairs, title, ylabel, log_scale)\n",
    "    plots = [\n",
    "        (('pre_corr', 'post_corr'), 'Correlations', 'Max Correlation', False),\n",
    "        (('pre_norm_mean', 'pre_norm_std'), 'Normalized Means (Mean/Std)', 'Mean/Std Ratio', False),\n",
    "        (('pre_std', 'post_std'), 'Channel Stds', 'Std', True),\n",
    "        (('deriv_mean', 'deriv_std'), f\"f'(z)² - {config['activation'].upper()}\", \"f'(z)²\", True),\n",
    "        (('gamma0_mean',), 'Gamma 0 (γ₀)', 'γ₀ Value', False),\n",
    "        (('gamma1_mean',), 'Gamma 1 (γ₁)', 'γ₁ Value', False),\n",
    "        (('gamma2_mean',), 'Gamma 2 (γ₂)', 'γ₂ Value', False),\n",
    "        (('duplicated_fraction',), 'Duplicated Units Fraction', 'Fraction', False)\n",
    "    ]\n",
    "    \n",
    "    for ax_idx, (metrics_pair, title, ylabel, log_scale) in enumerate(plots):\n",
    "        ax = axes[ax_idx]\n",
    "        \n",
    "        for layer_idx in range(num_layers):\n",
    "            color = colors[layer_idx]\n",
    "            \n",
    "            # Extract metric data for this layer\n",
    "            metric1_data = [metrics_log[i][1][layer_idx][metrics_pair[0]] for i in range(len(metrics_log))]\n",
    "            \n",
    "            # Plot based on panel type\n",
    "            if ax_idx == 0:  # Correlations: pre/post activation\n",
    "                metric2_data = [metrics_log[i][1][layer_idx][metrics_pair[1]] for i in range(len(metrics_log))]\n",
    "                ax.plot(iterations, metric1_data, color=color, linestyle='-', alpha=0.8, linewidth=2, label=f'z{layer_idx+1}')\n",
    "                ax.plot(iterations, metric2_data, color=color, linestyle='--', alpha=0.8, linewidth=2, label=f'a{layer_idx+1}')\n",
    "            elif ax_idx == 1:  # Normalized means: mean/std of ratios  \n",
    "                metric2_data = [metrics_log[i][1][layer_idx][metrics_pair[1]] for i in range(len(metrics_log))]\n",
    "                ax.plot(iterations, metric1_data, color=color, linestyle='-', alpha=0.8, linewidth=2, label=f'Mean z{layer_idx+1}')\n",
    "                # ax.plot(iterations, metric2_data, color=color, linestyle=':', alpha=0.8, linewidth=2, label=f'Std z{layer_idx+1}')\n",
    "            elif ax_idx == 2:  # Channel stds: pre/post activation\n",
    "                metric2_data = [metrics_log[i][1][layer_idx][metrics_pair[1]] for i in range(len(metrics_log))]\n",
    "                ax.plot(iterations, metric1_data, color=color, linestyle='-', alpha=0.8, linewidth=2, label=f'z{layer_idx+1}')\n",
    "                ax.plot(iterations, metric2_data, color=color, linestyle='--', alpha=0.8, linewidth=2, label=f'a{layer_idx+1}')\n",
    "            elif ax_idx == 3:  # Derivatives: mean/std\n",
    "                metric2_data = [metrics_log[i][1][layer_idx][metrics_pair[1]] for i in range(len(metrics_log))]\n",
    "                ax.plot(iterations, metric1_data, color=color, linestyle='-', alpha=0.8, linewidth=2, label=f'Mean z{layer_idx+1}')\n",
    "                # ax.plot(iterations, metric2_data, color=color, linestyle=':', alpha=0.8, linewidth=2, label=f'Std z{layer_idx+1}')\n",
    "            elif ax_idx in [4, 5, 6]:  # Gamma 0, 1, 2\n",
    "                ax.plot(iterations, metric1_data, color=color, linestyle='-', alpha=0.8, linewidth=2, label=f'L{layer_idx+1}')\n",
    "            elif ax_idx == 7:  # Duplicated units fraction\n",
    "                ax.plot(iterations, metric1_data, color=color, linestyle='-', alpha=0.8, linewidth=2, label=f'L{layer_idx+1}')\n",
    "        \n",
    "        ax.set_title(title, fontweight='bold')\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        if ax_idx > 0:\n",
    "            ax.set_xscale('log')\n",
    "        \n",
    "        if log_scale:\n",
    "            ax.set_yscale('log')\n",
    "        \n",
    "        # Add legend to each plot for clarity\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config['save_path'], dpi=config['dpi'], bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def train_and_analyze(config):\n",
    "    # Setup\n",
    "    device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')\n",
    "    gamma_mode = config.get('gamma_mode', 'exact')\n",
    "    print(f\"Training {config['activation']} network on {config['dataset'].upper()} using {device}\")\n",
    "    print(f\"Using {gamma_mode} gamma calculation method\")\n",
    "    \n",
    "    # Dataset-specific configuration\n",
    "    if config['dataset'] == 'mnist':\n",
    "        input_size = 784  # 28*28*1\n",
    "        dataset_class = torchvision.datasets.MNIST\n",
    "        normalize_mean = (0.1307,)\n",
    "        normalize_std = (0.3081,)\n",
    "    elif config['dataset'] == 'cifar10':\n",
    "        input_size = 3072  # 32*32*3\n",
    "        dataset_class = torchvision.datasets.CIFAR10\n",
    "        normalize_mean = (0.5, 0.5, 0.5)\n",
    "        normalize_std = (0.5, 0.5, 0.5)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {config['dataset']}\")\n",
    "    \n",
    "    # Data loading\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(normalize_mean, normalize_std)\n",
    "    ])\n",
    "    trainset = dataset_class(root='./data', train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Model setup\n",
    "    model = MLP(input_size=input_size, hidden_sizes=config['hidden_sizes'], activation=config['activation']).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if config['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), \n",
    "                               lr=config['learning_rate'],\n",
    "                              weight_decay=config['weight_decay'], \n",
    "                              )\n",
    "    elif config['optimizer'] == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), \n",
    "                              lr=config['learning_rate'], \n",
    "                              weight_decay=config['weight_decay'], \n",
    "                              momentum=config['momentum'])\n",
    "    else:\n",
    "        raise ValueError(f\"optimizer {config['optimizer']} not found\")\n",
    "    \n",
    "    # Metrics tracking\n",
    "    metrics_log = [(0, calculate_metrics(model, trainloader, device, \n",
    "                                        gamma_mode=config.get('gamma_mode', 'exact'),\n",
    "                                        gh_nodes=config.get('gh_nodes', 160)))]  # Initial metrics\n",
    "    \n",
    "    # Training loop\n",
    "    iteration_count = 0\n",
    "    losses = []\n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(inputs), labels)\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "            \n",
    "            iteration_count += 1\n",
    "            \n",
    "            # Log metrics periodically\n",
    "            if iteration_count % config['calc_every_n_iterations'] == 0:\n",
    "                metrics = calculate_metrics(model, trainloader, device,\n",
    "                                          gamma_mode=config.get('gamma_mode', 'exact'),\n",
    "                                          gh_nodes=config.get('gh_nodes', 160))\n",
    "                metrics_log.append((iteration_count, metrics))\n",
    "                print(f\"Epoch {epoch+1}/{config['epochs']}, Iter {iteration_count}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        # End of epoch metrics\n",
    "        avg_loss = sum(losses)/ len(losses)\n",
    "        final_metrics = calculate_metrics(model, trainloader, device,\n",
    "                                        gamma_mode=config.get('gamma_mode', 'exact'),\n",
    "                                        gh_nodes=config.get('gh_nodes', 160))\n",
    "        metrics_log.append((iteration_count, final_metrics))\n",
    "        print(f\"Epoch {epoch+1} completed, Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Plotting and final summary\n",
    "    plot_metrics(metrics_log, config)\n",
    "    \n",
    "    print(f\"\\nFinal metrics for {len(config['hidden_sizes'])}-layer {config['activation']} network on {config['dataset'].upper()}:\")\n",
    "    final = metrics_log[-1][1]\n",
    "    for i in range(len(config['hidden_sizes'])):\n",
    "        m = final[i]\n",
    "        print(f\"Layer {i+1}: Corr(z/a)={m['pre_corr']:.3f}/{m['post_corr']:.3f}, \"\n",
    "              f\"Norm(z)={m['pre_norm_mean']:.3f}±{m['pre_norm_std']:.3f}, \"\n",
    "              f\"f'(z)²={m['deriv_mean']:.3f}±{m['deriv_std']:.3f}, \"\n",
    "              f\"γ=(γ0:{m['gamma0_mean']:.3f}, γ1:{m['gamma1_mean']:.3f}, γ2:{m['gamma2_mean']:.3f}), \"\n",
    "              f\"Dup={m['duplicated_fraction']:.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'dataset': 'mnist',  # Switch between 'mnist' and 'cifar10'\n",
    "        'hidden_sizes': [1024] * 5,\n",
    "        'activation': 'tanh',\n",
    "        'epochs': 100,\n",
    "        'batch_size': 512,\n",
    "        'optimizer': 'sgd',\n",
    "        'learning_rate': 0.1,\n",
    "        'momentum': 0.9,\n",
    "        'weight_decay': 0.001,\n",
    "        'calc_every_n_iterations': 1000,\n",
    "        'device': 'cuda:4',\n",
    "        'figsize': (24, 10),\n",
    "        'dpi': 150,\n",
    "        'save_path': 'correlation_analysis_with_gamma.png',\n",
    "        'gamma_mode': 'surrogate',  # 'exact' or 'surrogate'\n",
    "        'gh_nodes': 160  # Number of Gauss-Hermite nodes for surrogate mode\n",
    "    }\n",
    "    train_and_analyze(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f31027-70a5-4996-afb3-ca45cb7f29e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20938837-e61d-4eff-9d57-95701eae4672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
